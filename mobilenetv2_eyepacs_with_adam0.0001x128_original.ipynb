{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda0199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8002b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries are imported\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow\n",
    "from numpy.random import seed\n",
    "seed(1337)\n",
    "import datetime\n",
    "import time\n",
    "#from tensorflow import set_random_seed\n",
    "#set_random_seed(42)\n",
    "tensorflow.random.set_seed(42)\n",
    "from tensorflow.keras import optimizers\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from keras.optimizers import Adam\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import vgg16\n",
    "# from tensorflow.keras.applications.mobilenet_v3 import mobileNetV3\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img,img_to_array\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import layers, models, Model, optimizers\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, models, Model, optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "#from plot_confusion_matrix import plot_confusion_matrix\n",
    "print(\"All libraries are imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656f05eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training directory found\n",
      " validation directory found\n",
      " test directory found\n",
      "['level_0', 'level_1', 'level_2', 'level_3', 'level_4']\n",
      "test data samples 36\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = \"C:/DUAEFATIMA/eyepacs_original/train/\"\n",
    "print(\" training directory found\")\n",
    "\n",
    "val_data_dir = \"C:/DUAEFATIMA/eyepacs_original/val/\"\n",
    "print(\" validation directory found\")\n",
    "\n",
    "test_data_dir = \"C:/DUAEFATIMA/eyepacs_original/test/\"\n",
    "print(\" test directory found\")\n",
    "\n",
    "category_names = sorted(os.listdir(\"C:/DUAEFATIMA/APTOS/APTOS_preprocessed/APTOS_Process_Train/train\"))\n",
    "print(category_names)\n",
    "\n",
    "print(\"test data samples\",len(test_data_dir))\n",
    "nb_categories = len(category_names)\n",
    "print(nb_categories)\n",
    "\n",
    "img_pr_val =[]\n",
    "img_pr_test =[]\n",
    "img_pr_cat=[]\n",
    "\n",
    "for category in category_names:\n",
    "    #folder = '/home/ubuntuos/Documents/Diabetic Retinopathy/Augmented data/train/' + '/' + category\n",
    "    folderTrain = 'C:/DUAEFATIMA/eyepacs_original/train'+ '/'+ category\n",
    "    img_pr_cat.append(len(os.listdir(folderTrain)))\n",
    "    folderValidation = 'C:/DUAEFATIMA/eyepacs_original/val' + '/' + category\n",
    "    img_pr_val.append(len(os.listdir(folderValidation)))\n",
    "    folderTest = test_data_dir + '/' + category\n",
    "    img_pr_test.append(len(os.listdir(folderTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e030f049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_224\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "IMG_SHAPE = (image_size, image_size, 3)\n",
    "\n",
    "#Create the base model from the pre-trained model mobilenetv2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                              include_top=False,\n",
    "                                              weights='imagenet')\n",
    "#base_model.trainable = False \n",
    "#We should also prevent the weights of the convolution from being updated before the model is compiled and trained.\n",
    "#To do this we set the trainable attribute to false.\n",
    "base_model.summary()\n",
    "# base_model = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n",
    "#                                               include_top=False,\n",
    "#                                               weights='imagenet')\n",
    "# #base_model.trainable = False \n",
    "# #We should also prevent the weights of the convolution from being updated before the model is compiled and trained.\n",
    "# #To do this we set the trainable attribute to false.\n",
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0b930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                          base_model,\n",
    "                          keras.layers.GlobalAveragePooling2D(),\n",
    "                          keras.layers.Flatten(),\n",
    "                         \n",
    "                          keras.layers.Dense(32,activation='relu'),\n",
    "                         \n",
    "                          keras.layers.Dense(16,activation='relu'),\n",
    "                          keras.layers.Dense(5, activation='softmax')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3065c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                40992     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,299,589\n",
      "Trainable params: 2,265,477\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "# model.compile(\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f09ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images for \"training\":\n",
      "Found 34477 images belonging to 5 classes.\n",
      "Total number of images for \"validation\":\n",
      "Found 9849 images belonging to 5 classes.\n",
      "Total number of images for \"testing\":\n",
      "Found 4923 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "img_height, img_width = 224,224\n",
    "# the no. imgaes to load at each iteration\n",
    "batch_size = 128\n",
    "# only rescaling\n",
    "train_datagen =  ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    zoom_range=0.2,\n",
    "   brightness_range=[0.2,1.0], featurewise_center=True,\n",
    "    featurewise_std_normalization=True\n",
    ")\n",
    "test_datagen =  ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "   # brightness_range=[0.2,1.0], featurewise_center=True,\n",
    "    #featurewise_std_normalization=True\n",
    ")\n",
    "# these are generators for train/test data that will read pictures #found in the defined subfolders of 'data/'\n",
    "print('Total number of images for \"training\":')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"categorical\"\n",
    "    ,shuffle = True\n",
    "    #,color_mode='grayscale'\n",
    ")\n",
    "total_train = train_generator.n\n",
    "\n",
    "#train_generator = train_generator.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "print('Total number of images for \"validation\":')\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "val_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size,\n",
    "class_mode = \"categorical\"\n",
    "    ,#color_mode='grayscale',\n",
    "shuffle=False\n",
    ")\n",
    "total_val = val_generator.n\n",
    "print('Total number of images for \"testing\":')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "test_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size,\n",
    "class_mode = \"categorical\"\n",
    "    ,#color_mode='grayscale',\n",
    "shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "076bd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path=\"C:/DUAEFATIMA/batchsizes_experiments/128_batchsize/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ccf5a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Training Started at:  2022-10-14 09:45:37.875433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DUA E FATIMA\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\preprocessing\\image.py:1863: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DUA E FATIMA\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\preprocessing\\image.py:1873: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.9873 - accuracy: 0.5816\n",
      "Epoch 1: val_accuracy improved from -inf to 0.22662, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 227s 756ms/step - loss: 0.9873 - accuracy: 0.5816 - val_loss: 8.5153 - val_accuracy: 0.2266 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.6544 - accuracy: 0.7257\n",
      "Epoch 2: val_accuracy improved from 0.22662 to 0.24074, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 179s 640ms/step - loss: 0.6544 - accuracy: 0.7257 - val_loss: 6.8708 - val_accuracy: 0.2407 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.4983 - accuracy: 0.7975\n",
      "Epoch 3: val_accuracy did not improve from 0.24074\n",
      "270/270 [==============================] - 181s 645ms/step - loss: 0.4983 - accuracy: 0.7975 - val_loss: 8.3026 - val_accuracy: 0.2397 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.8538\n",
      "Epoch 4: val_accuracy improved from 0.24074 to 0.26185, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 189s 668ms/step - loss: 0.3727 - accuracy: 0.8538 - val_loss: 6.5673 - val_accuracy: 0.2619 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.8982\n",
      "Epoch 5: val_accuracy improved from 0.26185 to 0.28511, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 189s 679ms/step - loss: 0.2691 - accuracy: 0.8982 - val_loss: 7.1722 - val_accuracy: 0.2851 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9255\n",
      "Epoch 6: val_accuracy improved from 0.28511 to 0.29668, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 186s 663ms/step - loss: 0.2035 - accuracy: 0.9255 - val_loss: 6.9025 - val_accuracy: 0.2967 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1547 - accuracy: 0.9446\n",
      "Epoch 7: val_accuracy did not improve from 0.29668\n",
      "270/270 [==============================] - 185s 660ms/step - loss: 0.1547 - accuracy: 0.9446 - val_loss: 9.8989 - val_accuracy: 0.2531 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9583\n",
      "Epoch 8: val_accuracy did not improve from 0.29668\n",
      "270/270 [==============================] - 187s 660ms/step - loss: 0.1158 - accuracy: 0.9583 - val_loss: 9.2985 - val_accuracy: 0.2488 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9651\n",
      "Epoch 9: val_accuracy did not improve from 0.29668\n",
      "270/270 [==============================] - 189s 666ms/step - loss: 0.1003 - accuracy: 0.9651 - val_loss: 10.5764 - val_accuracy: 0.2539 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9715\n",
      "Epoch 10: val_accuracy did not improve from 0.29668\n",
      "270/270 [==============================] - 189s 670ms/step - loss: 0.0816 - accuracy: 0.9715 - val_loss: 8.1307 - val_accuracy: 0.2870 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9744\n",
      "Epoch 11: val_accuracy improved from 0.29668 to 0.33536, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 191s 673ms/step - loss: 0.0727 - accuracy: 0.9744 - val_loss: 6.5364 - val_accuracy: 0.3354 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9793\n",
      "Epoch 12: val_accuracy improved from 0.33536 to 0.47406, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 189s 674ms/step - loss: 0.0594 - accuracy: 0.9793 - val_loss: 4.0105 - val_accuracy: 0.4741 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9825\n",
      "Epoch 13: val_accuracy improved from 0.47406 to 0.54726, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 188s 673ms/step - loss: 0.0520 - accuracy: 0.9825 - val_loss: 3.0509 - val_accuracy: 0.5473 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9827\n",
      "Epoch 14: val_accuracy did not improve from 0.54726\n",
      "270/270 [==============================] - 184s 658ms/step - loss: 0.0505 - accuracy: 0.9827 - val_loss: 3.1571 - val_accuracy: 0.5439 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9837\n",
      "Epoch 15: val_accuracy improved from 0.54726 to 0.66118, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 192s 679ms/step - loss: 0.0454 - accuracy: 0.9837 - val_loss: 1.7206 - val_accuracy: 0.6612 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9860\n",
      "Epoch 16: val_accuracy improved from 0.66118 to 0.72637, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 186s 665ms/step - loss: 0.0418 - accuracy: 0.9860 - val_loss: 1.3702 - val_accuracy: 0.7264 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9864\n",
      "Epoch 17: val_accuracy improved from 0.72637 to 0.78231, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 188s 672ms/step - loss: 0.0385 - accuracy: 0.9864 - val_loss: 0.9322 - val_accuracy: 0.7823 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9882\n",
      "Epoch 18: val_accuracy did not improve from 0.78231\n",
      "270/270 [==============================] - 187s 667ms/step - loss: 0.0353 - accuracy: 0.9882 - val_loss: 1.9064 - val_accuracy: 0.6643 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9900\n",
      "Epoch 19: val_accuracy did not improve from 0.78231\n",
      "270/270 [==============================] - 189s 667ms/step - loss: 0.0307 - accuracy: 0.9900 - val_loss: 1.1486 - val_accuracy: 0.7619 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9889\n",
      "Epoch 20: val_accuracy did not improve from 0.78231\n",
      "270/270 [==============================] - 189s 665ms/step - loss: 0.0324 - accuracy: 0.9889 - val_loss: 1.0848 - val_accuracy: 0.7688 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9895\n",
      "Epoch 21: val_accuracy improved from 0.78231 to 0.83877, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 190s 672ms/step - loss: 0.0302 - accuracy: 0.9895 - val_loss: 0.6319 - val_accuracy: 0.8388 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9900\n",
      "Epoch 22: val_accuracy did not improve from 0.83877\n",
      "270/270 [==============================] - 187s 668ms/step - loss: 0.0303 - accuracy: 0.9900 - val_loss: 1.0606 - val_accuracy: 0.7786 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9911\n",
      "Epoch 23: val_accuracy improved from 0.83877 to 0.84668, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 192s 678ms/step - loss: 0.0267 - accuracy: 0.9911 - val_loss: 0.6366 - val_accuracy: 0.8467 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9910\n",
      "Epoch 24: val_accuracy did not improve from 0.84668\n",
      "270/270 [==============================] - 199s 711ms/step - loss: 0.0277 - accuracy: 0.9910 - val_loss: 0.8011 - val_accuracy: 0.8074 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9900\n",
      "Epoch 25: val_accuracy did not improve from 0.84668\n",
      "270/270 [==============================] - 196s 690ms/step - loss: 0.0285 - accuracy: 0.9900 - val_loss: 0.6785 - val_accuracy: 0.8446 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9913\n",
      "Epoch 26: val_accuracy improved from 0.84668 to 0.89390, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 196s 712ms/step - loss: 0.0263 - accuracy: 0.9913 - val_loss: 0.4066 - val_accuracy: 0.8939 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9920\n",
      "Epoch 27: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 196s 699ms/step - loss: 0.0235 - accuracy: 0.9920 - val_loss: 0.3879 - val_accuracy: 0.8910 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9926\n",
      "Epoch 28: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 196s 690ms/step - loss: 0.0217 - accuracy: 0.9926 - val_loss: 0.5032 - val_accuracy: 0.8682 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9906\n",
      "Epoch 29: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 196s 694ms/step - loss: 0.0257 - accuracy: 0.9906 - val_loss: 1.0892 - val_accuracy: 0.7909 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9906\n",
      "Epoch 30: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 193s 682ms/step - loss: 0.0270 - accuracy: 0.9906 - val_loss: 1.7476 - val_accuracy: 0.6990 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9936\n",
      "Epoch 31: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 193s 684ms/step - loss: 0.0184 - accuracy: 0.9936 - val_loss: 1.5232 - val_accuracy: 0.7158 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9929\n",
      "Epoch 32: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 195s 690ms/step - loss: 0.0209 - accuracy: 0.9929 - val_loss: 0.9073 - val_accuracy: 0.8015 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9929\n",
      "Epoch 33: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 197s 696ms/step - loss: 0.0205 - accuracy: 0.9929 - val_loss: 1.0829 - val_accuracy: 0.7887 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9929\n",
      "Epoch 34: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 192s 678ms/step - loss: 0.0206 - accuracy: 0.9929 - val_loss: 1.3188 - val_accuracy: 0.7537 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9929\n",
      "Epoch 35: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 191s 674ms/step - loss: 0.0197 - accuracy: 0.9929 - val_loss: 0.7273 - val_accuracy: 0.8395 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9938\n",
      "Epoch 36: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 191s 676ms/step - loss: 0.0189 - accuracy: 0.9938 - val_loss: 0.7684 - val_accuracy: 0.8258 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9923\n",
      "Epoch 37: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 193s 683ms/step - loss: 0.0224 - accuracy: 0.9923 - val_loss: 0.8507 - val_accuracy: 0.8015 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9938\n",
      "Epoch 38: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 196s 693ms/step - loss: 0.0178 - accuracy: 0.9938 - val_loss: 0.9983 - val_accuracy: 0.7910 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9941\n",
      "Epoch 39: val_accuracy did not improve from 0.89390\n",
      "270/270 [==============================] - 191s 675ms/step - loss: 0.0163 - accuracy: 0.9941 - val_loss: 0.5224 - val_accuracy: 0.8673 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9946\n",
      "Epoch 40: val_accuracy improved from 0.89390 to 0.90517, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 192s 677ms/step - loss: 0.0147 - accuracy: 0.9946 - val_loss: 0.3917 - val_accuracy: 0.9052 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9957\n",
      "Epoch 41: val_accuracy did not improve from 0.90517\n",
      "270/270 [==============================] - 189s 677ms/step - loss: 0.0131 - accuracy: 0.9957 - val_loss: 0.4143 - val_accuracy: 0.8897 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9941\n",
      "Epoch 42: val_accuracy did not improve from 0.90517\n",
      "270/270 [==============================] - 191s 675ms/step - loss: 0.0173 - accuracy: 0.9941 - val_loss: 0.4398 - val_accuracy: 0.8902 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9942\n",
      "Epoch 43: val_accuracy did not improve from 0.90517\n",
      "270/270 [==============================] - 194s 684ms/step - loss: 0.0158 - accuracy: 0.9942 - val_loss: 0.4922 - val_accuracy: 0.8806 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9949\n",
      "Epoch 44: val_accuracy did not improve from 0.90517\n",
      "270/270 [==============================] - 190s 670ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.4591 - val_accuracy: 0.8834 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9943\n",
      "Epoch 45: val_accuracy did not improve from 0.90517\n",
      "270/270 [==============================] - 191s 673ms/step - loss: 0.0171 - accuracy: 0.9943 - val_loss: 0.7636 - val_accuracy: 0.8399 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9942\n",
      "Epoch 46: val_accuracy did not improve from 0.90517\n",
      "270/270 [==============================] - 190s 673ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.4830 - val_accuracy: 0.8945 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9946\n",
      "Epoch 47: val_accuracy improved from 0.90517 to 0.91177, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 196s 694ms/step - loss: 0.0160 - accuracy: 0.9946 - val_loss: 0.3564 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9945\n",
      "Epoch 48: val_accuracy did not improve from 0.91177\n",
      "270/270 [==============================] - 188s 670ms/step - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.4937 - val_accuracy: 0.8898 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9955\n",
      "Epoch 49: val_accuracy did not improve from 0.91177\n",
      "270/270 [==============================] - 190s 669ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.6609 - val_accuracy: 0.8622 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9946\n",
      "Epoch 50: val_accuracy did not improve from 0.91177\n",
      "270/270 [==============================] - 189s 668ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 0.4794 - val_accuracy: 0.8905 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9961\n",
      "Epoch 51: val_accuracy did not improve from 0.91177\n",
      "270/270 [==============================] - 188s 665ms/step - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.4815 - val_accuracy: 0.8939 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9942\n",
      "Epoch 52: val_accuracy did not improve from 0.91177\n",
      "270/270 [==============================] - 192s 679ms/step - loss: 0.0179 - accuracy: 0.9942 - val_loss: 0.8464 - val_accuracy: 0.8223 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9952\n",
      "Epoch 53: val_accuracy did not improve from 0.91177\n",
      "270/270 [==============================] - 188s 663ms/step - loss: 0.0127 - accuracy: 0.9952 - val_loss: 0.9273 - val_accuracy: 0.8132 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9931\n",
      "Epoch 54: val_accuracy did not improve from 0.91177\n",
      "270/270 [==============================] - 191s 674ms/step - loss: 0.0193 - accuracy: 0.9931 - val_loss: 1.2830 - val_accuracy: 0.7694 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9966\n",
      "Epoch 55: val_accuracy did not improve from 0.91177\n",
      "270/270 [==============================] - 191s 674ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.5851 - val_accuracy: 0.8746 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9967\n",
      "Epoch 56: val_accuracy did not improve from 0.91177\n",
      "270/270 [==============================] - 190s 670ms/step - loss: 0.0100 - accuracy: 0.9967 - val_loss: 0.4354 - val_accuracy: 0.8969 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9963\n",
      "Epoch 57: val_accuracy improved from 0.91177 to 0.91674, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 196s 693ms/step - loss: 0.0108 - accuracy: 0.9963 - val_loss: 0.3343 - val_accuracy: 0.9167 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9956\n",
      "Epoch 58: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 188s 672ms/step - loss: 0.0121 - accuracy: 0.9956 - val_loss: 0.5159 - val_accuracy: 0.8828 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9957\n",
      "Epoch 59: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 190s 669ms/step - loss: 0.0140 - accuracy: 0.9957 - val_loss: 0.3494 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9949\n",
      "Epoch 60: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 189s 669ms/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.5229 - val_accuracy: 0.8791 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9946\n",
      "Epoch 61: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 189s 668ms/step - loss: 0.0140 - accuracy: 0.9946 - val_loss: 0.6095 - val_accuracy: 0.8658 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9961\n",
      "Epoch 62: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 192s 674ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.4693 - val_accuracy: 0.8898 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9965\n",
      "Epoch 63: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 190s 673ms/step - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.4345 - val_accuracy: 0.8967 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9972\n",
      "Epoch 64: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 189s 667ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.5586 - val_accuracy: 0.8733 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 65: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 188s 665ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.4055 - val_accuracy: 0.9049 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9960\n",
      "Epoch 66: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 192s 676ms/step - loss: 0.0114 - accuracy: 0.9960 - val_loss: 0.4319 - val_accuracy: 0.9053 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9958\n",
      "Epoch 67: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 190s 672ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.5983 - val_accuracy: 0.8781 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n",
      "Epoch 68: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 190s 670ms/step - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.4808 - val_accuracy: 0.8905 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9956\n",
      "Epoch 69: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 189s 668ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.7024 - val_accuracy: 0.8627 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9965\n",
      "Epoch 70: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 190s 669ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.6719 - val_accuracy: 0.8552 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9972\n",
      "Epoch 71: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 192s 677ms/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 0.7296 - val_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9972\n",
      "Epoch 72: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 191s 674ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.5030 - val_accuracy: 0.8839 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9944\n",
      "Epoch 73: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 190s 670ms/step - loss: 0.0155 - accuracy: 0.9944 - val_loss: 0.6331 - val_accuracy: 0.8605 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9968\n",
      "Epoch 74: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 188s 663ms/step - loss: 0.0097 - accuracy: 0.9968 - val_loss: 0.7524 - val_accuracy: 0.8494 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9975\n",
      "Epoch 75: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 188s 665ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.6437 - val_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9958\n",
      "Epoch 76: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 193s 681ms/step - loss: 0.0120 - accuracy: 0.9958 - val_loss: 0.4961 - val_accuracy: 0.8912 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9966\n",
      "Epoch 77: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 190s 672ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.4997 - val_accuracy: 0.8856 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9977\n",
      "Epoch 78: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 190s 671ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.5690 - val_accuracy: 0.8750 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9970\n",
      "Epoch 79: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 190s 673ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.5257 - val_accuracy: 0.8828 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9963\n",
      "Epoch 80: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 194s 688ms/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.4365 - val_accuracy: 0.9058 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9966\n",
      "Epoch 81: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 193s 681ms/step - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.7044 - val_accuracy: 0.8545 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9966\n",
      "Epoch 82: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 192s 678ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.4203 - val_accuracy: 0.8989 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.9965\n",
      "Epoch 83: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 191s 674ms/step - loss: 0.0099 - accuracy: 0.9965 - val_loss: 0.7926 - val_accuracy: 0.8476 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9970\n",
      "Epoch 84: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 191s 675ms/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 0.4279 - val_accuracy: 0.9073 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9972\n",
      "Epoch 85: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 194s 686ms/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.5918 - val_accuracy: 0.8826 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9963\n",
      "Epoch 86: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 193s 682ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.6694 - val_accuracy: 0.8638 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9962\n",
      "Epoch 87: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 193s 680ms/step - loss: 0.0108 - accuracy: 0.9962 - val_loss: 0.4882 - val_accuracy: 0.8977 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9980\n",
      "Epoch 88: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 189s 668ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.7032 - val_accuracy: 0.8581 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9969\n",
      "Epoch 89: val_accuracy did not improve from 0.91674\n",
      "270/270 [==============================] - 192s 677ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.3627 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9963\n",
      "Epoch 90: val_accuracy improved from 0.91674 to 0.94416, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 197s 696ms/step - loss: 0.0113 - accuracy: 0.9963 - val_loss: 0.2267 - val_accuracy: 0.9442 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9973\n",
      "Epoch 91: val_accuracy did not improve from 0.94416\n",
      "270/270 [==============================] - 191s 682ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.3480 - val_accuracy: 0.9155 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9972\n",
      "Epoch 92: val_accuracy improved from 0.94416 to 0.94670, saving model to C:/DUAEFATIMA/batchsizes_experiments/128_batchsize\\mobilenetv2_128.h5\n",
      "270/270 [==============================] - 193s 683ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.1976 - val_accuracy: 0.9467 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9967\n",
      "Epoch 93: val_accuracy did not improve from 0.94670\n",
      "270/270 [==============================] - 187s 669ms/step - loss: 0.0085 - accuracy: 0.9967 - val_loss: 0.5323 - val_accuracy: 0.8806 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 94: val_accuracy did not improve from 0.94670\n",
      "270/270 [==============================] - 191s 676ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.2788 - val_accuracy: 0.9303 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9981\n",
      "Epoch 95: val_accuracy did not improve from 0.94670\n",
      "270/270 [==============================] - 192s 675ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.4081 - val_accuracy: 0.9039 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9961\n",
      "Epoch 96: val_accuracy did not improve from 0.94670\n",
      "270/270 [==============================] - 191s 673ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.4323 - val_accuracy: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9962\n",
      "Epoch 97: val_accuracy did not improve from 0.94670\n",
      "270/270 [==============================] - 191s 672ms/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 0.4877 - val_accuracy: 0.8954 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9963\n",
      "Epoch 98: val_accuracy did not improve from 0.94670\n",
      "270/270 [==============================] - 191s 676ms/step - loss: 0.0104 - accuracy: 0.9963 - val_loss: 0.3320 - val_accuracy: 0.9283 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9970\n",
      "Epoch 99: val_accuracy did not improve from 0.94670\n",
      "270/270 [==============================] - 193s 684ms/step - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.4521 - val_accuracy: 0.9015 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9979\n",
      "Epoch 100: val_accuracy did not improve from 0.94670\n",
      "270/270 [==============================] - 190s 671ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.2492 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Time:  5:18:47.149710\n"
     ]
    }
   ],
   "source": [
    "# main_model_dir=r\"C:/DUAEFATIMA/output\"\n",
    "# model_dir = main_model_dir + time.strftime('%Y-%m-%d %H-%M-%S') + \"/\"\n",
    "# model_file = model_dir + \"{epoch:02d}-val_accuracy-{val_accuracy:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "epochs = 100\n",
    "checkpoint = ModelCheckpoint(\"C:/DUAEFATIMA/batchsizes_experiments/128_batchsize/mobilenetv2_128.h5\", monitor = 'val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=80, verbose=1, mode='max', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=0.001)\n",
    "# steps_per_epoch = \n",
    "# validation_steps = 100\n",
    "csv_logger = CSVLogger('C:/DUAEFATIMA/batchsizes_experiments/128_batchsize/mobilenetv2_128.csv', separator=',')\n",
    "\n",
    "print(\"Training Started at: \",start)\n",
    "history = model.fit(train_generator,\n",
    "                              #steps_per_epoch = 120,\n",
    "                              epochs=epochs,\n",
    "                              workers=12,\n",
    "                              #validation_steps=120,\n",
    "                              validation_data=val_generator, verbose = 1,callbacks = [csv_logger,early, reduce_lr,checkpoint]\n",
    "                               #validation_data=val_generator, verbose = 1,callbacks = [early, reduce_lr,checkpoint]\n",
    "                             )\n",
    "end = datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print (\"Time: \", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903a196",
   "metadata": {},
   "source": [
    "for key in historyCallback.history:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25271fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:/DUAEFATIMA/batchsizes_experiments/128_batchsize/mobilenetv2_128final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1ac9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "legends = ['Training', 'Validation']\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label = 'Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(legends, loc='upper left')\n",
    "plt.savefig('C:/DUAEFATIMA/batchsizes_experiments/128_batchsize/mobilenetv2_128acc.jpg')\n",
    "\n",
    "# plt.plot(epochs, loss, 'g', label = 'Training loss')\n",
    "# plt.plot(epochs, val_loss, 'y', label='Validation loss')\n",
    "\n",
    "# plt.savefig('C:/DUAEFATIMA/Loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e06f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "legends = ['Training', 'Validation']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, loss, 'b', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(legends, loc='upper left')\n",
    "plt.savefig('C:/DUAEFATIMA/batchsizes_experiments/128_batchsize/mobilenetv2_128Loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03206887",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2b47351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 68s 2s/step\n",
      "[0 0 0 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print(y_pred)\n",
    "#cm = confusion_matrix(test_generator, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaaed828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict_generator(test_generator.classes)\n",
    "# # Get most likely class\n",
    "# predicted_classes = np.argmax(y_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84cb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report = classification_report(true_classes, predicted_classes, target_names=names)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a9c79ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 68s 2s/step\n",
      "[[1099    2   11    2    1]\n",
      " [ 100  440  410    9    9]\n",
      " [  29   41  869   35   25]\n",
      " [   9   12  207  645   55]\n",
      " [  10    0   60   26  817]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57403537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity [0.98565022 0.45454545 0.86986987 0.6950431  0.89485214]\n",
      "Specificity [0.96113445 0.98609355 0.82466871 0.98197747 0.97755611]\n",
      "Precision or Positive Predictive Values [0.88131516 0.88888889 0.5581246  0.89958159 0.90077178]\n",
      "Negative predictive values [0.99564744 0.88075881 0.96137849 0.93271517 0.97609562]\n",
      "False Positive or Fall Out [0.03886555 0.01390645 0.17533129 0.01802253 0.02244389]\n",
      "False Negative Rate [0.01434978 0.54545455 0.13013013 0.3049569  0.10514786]\n",
      "False discovery Rate [0.11868484 0.11111111 0.4418754  0.10041841 0.09922822]\n",
      "Accuracy [0.96668698 0.88157627 0.83384115 0.9278895  0.96221816]\n"
     ]
    }
   ],
   "source": [
    "FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print(\"Sensitivity\",TPR)\n",
    "print(\"Specificity\",TNR)\n",
    "print(\"Precision or Positive Predictive Values\",PPV)\n",
    "print(\"Negative predictive values\",NPV)\n",
    "print(\"False Positive or Fall Out\",FPR)\n",
    "print(\"False Negative Rate\",FNR)\n",
    "print(\"False discovery Rate\",FDR)\n",
    "print(\"Accuracy\",ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d90c1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test set: 78.6% \n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_generator.classes, y_pred)\n",
    "print(\"Accuracy in test set: %0.1f%% \" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be4c01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4368a49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ce4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b6a3985",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         c_ax\u001b[38;5;241m.\u001b[39mplot(FPR, FPR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb-\u001b[39m\u001b[38;5;124m'\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Guessing\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m (c_label, auc(FPR, TPR)))\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m roc_auc_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39maverage)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROC AUC score:\u001b[39m\u001b[38;5;124m'\u001b[39m, multiclass_roc_auc_score(test_generator\u001b[38;5;241m.\u001b[39mclasses, \u001b[43my_pred\u001b[49m,average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     32\u001b[0m c_ax\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m     33\u001b[0m c_ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse Positive Rate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "target= ['level_0', 'level_1', 'level_2', 'level_3', 'level_4']\n",
    "\n",
    "# set plot figure size\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "\n",
    "# function for scoring roc auc score for multi-class\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "\n",
    "    for (idx, c_label) in enumerate(target):\n",
    "        FPR, TPR, thresholds = roc_curve(y_test[:,idx].astype(int),y_pred)\n",
    "        c_ax.plot(FPR, label = '%s (AUC:%0.2f)'  % (c_label, auc(FPR, TPR)))\n",
    "        c_ax.plot(FPR, FPR, 'b-', label = 'Random Guessing'% (c_label, auc(FPR, TPR)))\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "print('ROC AUC score:', multiclass_roc_auc_score(test_generator.classes, y_pred,average=\"macro\"))\n",
    "\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b65d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39939b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff47c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0fd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
